# 1、概述

## AirGym-master 

老版本airsim，改为新版本后一直有问题, responses = self.simGetImages经常为0，不改了，原理已经验证。

## openai_drone_gym-master

改的AirGym-master，应该是新版本，没跑过。

## DRLDBackEnd

AirGym的升级版，用的老版本airsim，用老版本的blocks环境能跑，存在问题。

## DroneTracking

改的DRLDBackEnd，新版本，我对images的理解有问题，待进一步理解后研究。
该作者还有其他资料。





Airsim_RL-master PPO，新版本的Airsim，可以跑，而且也是keras，只不过不是keras-rl，跑的一直不出结果，也不见有改进。

ppo 都改了什么


# 2、训练记录

直接在复杂环境中训练，相当于让一个婴儿直接学泛函，应该首先降低环境复杂度，或者将目标分解，逐个通过加载模型的方式进行训练。

在室内避障时，先去掉室内所有障碍，训练目标方向，然后加上室内障碍，但是同高度，训练避障，然后调整窗户高度，训练高低维度。

**20190814**
模型能够停下来观察了，而不是一直滑行；

但在起飞区域一直徘徊，甚至飞进窗户后又飞出来。这是不是因为奖励中我设置的时间系数低了，模型变得太保守，因为距离比AirGym要近所以设为1---0.1了，改为0.5尝试。是不是应该先把drone送进区域，起飞区域没有障碍，他看到的只是空白，会一直为避障而在起飞区域徘徊，因为避障的惩罚太大了100，而时间惩罚我设置的才为【0.1，1】，距离奖励也是个位数以内。**所要训练的期望要简单，起飞区域的空白太大，时间与距离的诱导又太小** 。

**20190816**

原来的环境是150*225 比我的要大， 10*25
目前改动：
针对不向里走的问题，放大时间惩罚与靠近目标奖励的系数，

时间惩罚为-2，鼓励尽快完成

进步*5，鼓励向前走

减小碰撞惩罚为-20

**20190817**

三维下的训练达不到好的结果，到处转在比赛时是不行的，必须按照目标方向一直向前，而且有个停不下来的问题，这个通过训练应该是可以克服的。因为现在模型已经能够停下来观察了。

由于比赛的高度事先可以知道，所以重新回到3个动作空间的情况，后续改动就是 把停下来也作为一个动作，是不是会提高速度，不停的时候就不用停，会快一些。

action从6改为3之后，原来训练的模型就没法用了，提示：
ValueError: Dimension 1 in both shapes must be equal, but are 3 and 6. Shapes are [256,3] and [256,6]. for 'Assign_22' (op: 'Assign') with input shapes: [256,3], [256,6].
